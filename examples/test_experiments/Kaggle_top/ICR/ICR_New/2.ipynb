{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reference : https://www.kaggle.com/code/aikhmelnytskyy/public-krni-pdi-with-two-additional-models\n# Description: Added some comments in the referenced code.\n# Please vote if it was helpful to you. :)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T10:08:23.66068Z","iopub.execute_input":"2023-06-25T10:08:23.661832Z","iopub.status.idle":"2023-06-25T10:08:44.412009Z","shell.execute_reply.started":"2023-06-25T10:08:23.661787Z","shell.execute_reply":"2023-06-25T10:08:44.409631Z"}}},{"cell_type":"code","source":"!pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr/pip-packages\n!mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n!cp /kaggle/input/pip-packages-icr/pip-packages/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:32.44541Z","iopub.execute_input":"2023-07-02T05:48:32.445885Z","iopub.status.idle":"2023-07-02T05:48:51.349226Z","shell.execute_reply.started":"2023-07-02T05:48:32.445841Z","shell.execute_reply":"2023-07-02T05:48:51.34775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np                       # NumPy for numerical computations\nimport pandas as pd                      # Pandas for data manipulation and analysis\nfrom sklearn.preprocessing import LabelEncoder, normalize   # LabelEncoder for encoding categorical variables, normalize for feature scaling\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier   # GradientBoostingClassifier and RandomForestClassifier for classification models\nfrom sklearn.metrics import accuracy_score   # accuracy_score for evaluating model performance\nfrom sklearn.impute import SimpleImputer   # SimpleImputer for handling missing values\nimport imblearn   # imblearn for imbalanced dataset handling\nfrom imblearn.over_sampling import RandomOverSampler   # RandomOverSampler for oversampling minority class\nfrom imblearn.under_sampling import RandomUnderSampler   # RandomUnderSampler for undersampling majority class\nimport xgboost   # XGBoost for gradient boosting models\nimport inspect   # inspect for retrieving information about live objects\nfrom collections import defaultdict   # defaultdict for creating a dictionary with default values\nfrom tabpfn import TabPFNClassifier   # TabPFNClassifier for a specific classification model\nimport warnings   # warnings for ignoring warnings during runtime","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-02T05:48:51.355331Z","iopub.execute_input":"2023-07-02T05:48:51.356015Z","iopub.status.idle":"2023-07-02T05:48:55.861919Z","shell.execute_reply.started":"2023-07-02T05:48:51.355973Z","shell.execute_reply":"2023-07-02T05:48:55.860938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\ntest = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\nsample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\ngreeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:55.863534Z","iopub.execute_input":"2023-07-02T05:48:55.864172Z","iopub.status.idle":"2023-07-02T05:48:55.914468Z","shell.execute_reply.started":"2023-07-02T05:48:55.864138Z","shell.execute_reply":"2023-07-02T05:48:55.913579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# lb = LabelEncoder()\n# train['EJ'] = lb.fit_transform(train['EJ']).astype(float)\n# test['EJ'] = lb.fit_transform(test['EJ']).astype(float)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:55.917231Z","iopub.execute_input":"2023-07-02T05:48:55.917627Z","iopub.status.idle":"2023-07-02T05:48:55.923024Z","shell.execute_reply.started":"2023-07-02T05:48:55.917592Z","shell.execute_reply":"2023-07-02T05:48:55.920953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign the first unique value of the 'EJ' column in the 'train' dataframe to the variable 'first_category'\nfirst_category = train.EJ.unique()[0]\n\n# Convert the values in the 'EJ' column of the 'train' dataframe to binary values (0 or 1),\n# where 1 represents the occurrence of the 'first_category' and 0 represents other categories\ntrain.EJ = train.EJ.eq(first_category).astype('int')\n\n# Convert the values in the 'EJ' column of the 'test' dataframe to binary values (0 or 1),\n# based on the occurrence of the 'first_category' in the 'train' dataframe\ntest.EJ = test.EJ.eq(first_category).astype('int')","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:55.924416Z","iopub.execute_input":"2023-07-02T05:48:55.925394Z","iopub.status.idle":"2023-07-02T05:48:55.944032Z","shell.execute_reply.started":"2023-07-02T05:48:55.925333Z","shell.execute_reply":"2023-07-02T05:48:55.943128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:55.945351Z","iopub.execute_input":"2023-07-02T05:48:55.945835Z","iopub.status.idle":"2023-07-02T05:48:55.961433Z","shell.execute_reply.started":"2023-07-02T05:48:55.945787Z","shell.execute_reply":"2023-07-02T05:48:55.960503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_under_sampler(df):\n    # Calculate the number of samples for each label. \n    neg, pos = np.bincount(df['Class'])\n\n    # Choose the samples with class label `1`.\n    one_df = df.loc[df['Class'] == 1] \n    # Choose the samples with class label `0`.\n    zero_df = df.loc[df['Class'] == 0]\n    # Select `pos` number of negative samples.\n    # This makes sure that we have equal number of samples for each label.\n    zero_df = zero_df.sample(n=pos)\n\n    # Join both label dataframes.\n    undersampled_df = pd.concat([zero_df, one_df])\n\n    # Shuffle the data and return\n    return undersampled_df.sample(frac = 1)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:55.962949Z","iopub.execute_input":"2023-07-02T05:48:55.96333Z","iopub.status.idle":"2023-07-02T05:48:55.971461Z","shell.execute_reply.started":"2023-07-02T05:48:55.963297Z","shell.execute_reply":"2023-07-02T05:48:55.970631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform random under-sampling on the 'train' dataset and assign the result to the variable 'train_good'\ntrain_good = random_under_sampler(train)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:55.973125Z","iopub.execute_input":"2023-07-02T05:48:55.973455Z","iopub.status.idle":"2023-07-02T05:48:55.986773Z","shell.execute_reply.started":"2023-07-02T05:48:55.973425Z","shell.execute_reply":"2023-07-02T05:48:55.985878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the shape of the 'train_good' dataset\ntrain_good.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:55.988019Z","iopub.execute_input":"2023-07-02T05:48:55.989967Z","iopub.status.idle":"2023-07-02T05:48:55.998318Z","shell.execute_reply.started":"2023-07-02T05:48:55.989943Z","shell.execute_reply":"2023-07-02T05:48:55.997408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list called 'predictor_columns' that contains all column names from the 'train' dataframe\n# except for the columns named 'Class' and 'Id'\npredictor_columns = [n for n in train.columns if n != 'Class' and n != 'Id']\n\n# Create a new dataframe 'x' that contains the subset of columns specified in the 'predictor_columns' list\nx = train[predictor_columns]\n\n# Create a new series 'y' that contains the values from the 'Class' column in the 'train' dataframe\ny = train['Class']","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.002899Z","iopub.execute_input":"2023-07-02T05:48:56.003209Z","iopub.status.idle":"2023-07-02T05:48:56.01215Z","shell.execute_reply.started":"2023-07-02T05:48:56.003185Z","shell.execute_reply":"2023-07-02T05:48:56.011026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_norm = np.array(x_norm)\n# y_ros = np.array(y_ros)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.013565Z","iopub.execute_input":"2023-07-02T05:48:56.013957Z","iopub.status.idle":"2023-07-02T05:48:56.027106Z","shell.execute_reply.started":"2023-07-02T05:48:56.013925Z","shell.execute_reply":"2023-07-02T05:48:56.02617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries for model evaluation and hyperparameter tuning\nfrom sklearn.model_selection import KFold as KF, GridSearchCV\n\n# Create an outer K-Fold cross-validation object with 10 splits\n# Shuffle the data before splitting and set the random state for reproducibility\ncv_outer = KF(n_splits=10, shuffle=True, random_state=42)\n\n# Create an inner K-Fold cross-validation object with 5 splits\n# Shuffle the data before splitting and set the random state for reproducibility\ncv_inner = KF(n_splits=5, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.028383Z","iopub.execute_input":"2023-07-02T05:48:56.028851Z","iopub.status.idle":"2023-07-02T05:48:56.038477Z","shell.execute_reply.started":"2023-07-02T05:48:56.028809Z","shell.execute_reply":"2023-07-02T05:48:56.037533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def balanced_log_loss(y_true, y_pred):\n    # y_true: correct labels 0, 1\n    # y_pred: predicted probabilities of class=1\n    # calculate the number of observations for each class\n    N_0 = np.sum(1 - y_true)\n    N_1 = np.sum(y_true)\n    # calculate the weights for each class to balance classes\n    w_0 = 1 / N_0\n    w_1 = 1 / N_1\n    # calculate the predicted probabilities for each class\n    p_1 = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    p_0 = 1 - p_1\n    # calculate the summed log loss for each class\n    log_loss_0 = -np.sum((1 - y_true) * np.log(p_0))\n    log_loss_1 = -np.sum(y_true * np.log(p_1))\n    # calculate the weighted summed logarithmic loss\n    # (factgor of 2 included to give same result as LL with balanced input)\n    balanced_log_loss = 2*(w_0 * log_loss_0 + w_1 * log_loss_1) / (w_0 + w_1)\n    # return the average log loss\n    return balanced_log_loss/(N_0+N_1)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.041179Z","iopub.execute_input":"2023-07-02T05:48:56.041526Z","iopub.status.idle":"2023-07-02T05:48:56.050975Z","shell.execute_reply.started":"2023-07-02T05:48:56.041503Z","shell.execute_reply":"2023-07-02T05:48:56.050101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Ensemble():\n    def __init__(self):\n        self.imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n\n        self.classifiers =[xgboost.XGBClassifier(n_estimators=100,max_depth=3,learning_rate=0.2,subsample=0.9,colsample_bytree=0.85),\n                          \n                           xgboost.XGBClassifier(),\n                           TabPFNClassifier(N_ensemble_configurations=24),\n                          \n                          TabPFNClassifier(N_ensemble_configurations=64)]\n    \n    def fit(self,X,y):\n        y = y.values\n        unique_classes, y = np.unique(y, return_inverse=True)\n        self.classes_ = unique_classes\n        first_category = X.EJ.unique()[0]\n        X.EJ = X.EJ.eq(first_category).astype('int')\n        X = self.imputer.fit_transform(X)\n#         X = normalize(X,axis=0)\n        for classifier in self.classifiers:\n            if classifier==self.classifiers[2] or classifier==self.classifiers[3]:\n                classifier.fit(X,y,overwrite_warning =True)\n            else :\n                classifier.fit(X, y)\n     \n    def predict_proba(self, x):\n        x = self.imputer.transform(x)\n#         x = normalize(x,axis=0)\n        probabilities = np.stack([classifier.predict_proba(x) for classifier in self.classifiers])\n        averaged_probabilities = np.mean(probabilities, axis=0)\n        class_0_est_instances = averaged_probabilities[:, 0].sum()\n        others_est_instances = averaged_probabilities[:, 1:].sum()\n        # Weighted probabilities based on class imbalance\n        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n        return new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1) ","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.05223Z","iopub.execute_input":"2023-07-02T05:48:56.053811Z","iopub.status.idle":"2023-07-02T05:48:56.066078Z","shell.execute_reply.started":"2023-07-02T05:48:56.053778Z","shell.execute_reply":"2023-07-02T05:48:56.065219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the 'tqdm' module from the 'tqdm.notebook' package\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.067692Z","iopub.execute_input":"2023-07-02T05:48:56.068035Z","iopub.status.idle":"2023-07-02T05:48:56.157171Z","shell.execute_reply.started":"2023-07-02T05:48:56.068004Z","shell.execute_reply":"2023-07-02T05:48:56.156152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training(model, x, y, y_meta):\n    outer_results = list()   # List to store the loss results of each outer fold\n    best_loss = np.inf   # Variable to track the best loss achieved during training\n    split = 0   # Counter for the current split\n    splits = 5   # Total number of splits for the inner cross-validation\n    models = []   # List to store the trained models for each inner fold\n\n    # Loop over the splits of the inner cross-validation using tqdm for progress visualization\n    for train_idx, val_idx in tqdm(cv_inner.split(x), total=splits):\n        split += 1   # Increment the split counter\n        x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]   # Split the training data into training and validation sets\n        y_train, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]   # Split the target variable into training and validation sets\n\n        model.fit(x_train, y_train)   # Fit the model on the training data\n        models.append(model)   # Append the trained model to the list of models\n\n        y_pred = model.predict_proba(x_val)   # Predict probabilities for the validation set\n        probabilities = np.concatenate((y_pred[:, :1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)   # Calculate class probabilities\n\n        p0 = probabilities[:, :1]   # Extract probabilities for the first class\n        p0[p0 > 0.86] = 1   # Set probabilities greater than 0.86 to 1 (True)\n        p0[p0 < 0.14] = 0   # Set probabilities less than 0.14 to 0 (False)\n\n        y_p = np.empty((y_pred.shape[0],))   # Create an empty array to store predicted labels\n\n        # Convert probabilities to binary labels based on a threshold of 0.5\n        for i in range(y_pred.shape[0]):\n            if p0[i] >= 0.5:\n                y_p[i] = False\n            else:\n                y_p[i] = True\n\n        y_p = y_p.astype(int)   # Convert the predicted labels to integer format\n        loss = balanced_log_loss(y_val, y_p)   # Calculate the balanced log loss between the predicted labels and the true labels\n\n        if loss < best_loss:\n            best_model = model   # Save the best model based on the lowest loss\n            best_loss = loss   # Update the best loss\n            print('best_model_saved')\n\n        outer_results.append(loss)   # Append the loss to the list of outer results\n        print('>val_loss=%.5f, split = %.1f' % (loss, split))\n\n    print('LOSS: %.5f' % (np.mean(outer_results)))   # Print the average loss across all outer folds\n    return best_model, models   # Return the best model and the list of trained models","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.158783Z","iopub.execute_input":"2023-07-02T05:48:56.159133Z","iopub.status.idle":"2023-07-02T05:48:56.17126Z","shell.execute_reply.started":"2023-07-02T05:48:56.1591Z","shell.execute_reply":"2023-07-02T05:48:56.170195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the 'datetime' module from the standard library\nfrom datetime import datetime\n\n# Create a copy of the 'Epsilon' column from the 'greeks' dataframe and assign it to the variable 'times'\ntimes = greeks.Epsilon.copy()\n\n# Replace non-'Unknown' values in the 'Epsilon' column with their corresponding ordinal date values\ntimes[greeks.Epsilon != 'Unknown'] = greeks.Epsilon[greeks.Epsilon != 'Unknown'].map(lambda x: datetime.strptime(x, '%m/%d/%Y').toordinal())\n\n# Replace 'Unknown' values in the 'Epsilon' column with NaN (missing value)\ntimes[greeks.Epsilon == 'Unknown'] = np.nan","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.172863Z","iopub.execute_input":"2023-07-02T05:48:56.173614Z","iopub.status.idle":"2023-07-02T05:48:56.200339Z","shell.execute_reply.started":"2023-07-02T05:48:56.173581Z","shell.execute_reply":"2023-07-02T05:48:56.199356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate the 'train' dataframe and the 'times' series along the columns (axis=1) and assign it to 'train_pred_and_time'\ntrain_pred_and_time = pd.concat((train, times), axis=1)\n\n# Select the predictor columns from the 'test' dataframe and assign it to 'test_predictors'\ntest_predictors = test[predictor_columns]\n\n# Get the first unique value from the 'EJ' column in the 'test_predictors' dataframe and assign it to 'first_category'\nfirst_category = test_predictors.EJ.unique()[0]\n\n# Convert the values in the 'EJ' column of the 'test_predictors' dataframe to binary values (0 or 1),\n# where 1 represents the occurrence of the 'first_category' and 0 represents other categories\ntest_predictors.EJ = test_predictors.EJ.eq(first_category).astype('int')\n\n# Create a new array 'test_pred_and_time' by concatenating 'test_predictors' and a column of zeros\n# with a shape of (len(test_predictors), 1). The column is adjusted to be greater than the maximum value in 'train_pred_and_time.Epsilon'\ntest_pred_and_time = np.concatenate((test_predictors, np.zeros((len(test_predictors), 1)) + train_pred_and_time.Epsilon.max() + 1), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.201638Z","iopub.execute_input":"2023-07-02T05:48:56.203114Z","iopub.status.idle":"2023-07-02T05:48:56.216287Z","shell.execute_reply.started":"2023-07-02T05:48:56.203074Z","shell.execute_reply":"2023-07-02T05:48:56.215181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a RandomOverSampler object with a random state of 42\nros = RandomOverSampler(random_state=42)\n\n# Resample the 'train_pred_and_time' dataframe and 'greeks.Alpha' series using RandomOverSampler\n# The resampled data is assigned to 'train_ros' and 'y_ros' respectively\ntrain_ros, y_ros = ros.fit_resample(train_pred_and_time, greeks.Alpha)\n\n# Print the value counts of the 'Alpha' column in the original dataset\nprint('Original dataset shape')\nprint(greeks.Alpha.value_counts())\n\n# Print the value counts of the 'y_ros' series in the resampled dataset\nprint('Resample dataset shape')\nprint(y_ros.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.217645Z","iopub.execute_input":"2023-07-02T05:48:56.21819Z","iopub.status.idle":"2023-07-02T05:48:56.262728Z","shell.execute_reply.started":"2023-07-02T05:48:56.218111Z","shell.execute_reply":"2023-07-02T05:48:56.261722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new dataframe 'x_ros' by dropping the 'Class' and 'Id' columns from the 'train_ros' dataframe\nx_ros = train_ros.drop(['Class', 'Id'], axis=1)\n\n# Assign the 'Class' column from the 'train_ros' dataframe to the variable 'y_'\ny_ = train_ros.Class","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.264264Z","iopub.execute_input":"2023-07-02T05:48:56.264625Z","iopub.status.idle":"2023-07-02T05:48:56.272039Z","shell.execute_reply.started":"2023-07-02T05:48:56.264591Z","shell.execute_reply":"2023-07-02T05:48:56.270909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yt = Ensemble()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:56.273642Z","iopub.execute_input":"2023-07-02T05:48:56.273972Z","iopub.status.idle":"2023-07-02T05:48:57.124724Z","shell.execute_reply.started":"2023-07-02T05:48:56.27394Z","shell.execute_reply":"2023-07-02T05:48:57.123622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the 'training' function with the input parameters 'yt', 'x_ros', 'y_', and 'y_ros'\n# The returned values 'm' and 'models' are assigned to the respective variables\nm, models = training(yt, x_ros, y_, y_ros)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T05:48:57.126287Z","iopub.execute_input":"2023-07-02T05:48:57.12673Z","iopub.status.idle":"2023-07-02T06:17:44.995177Z","shell.execute_reply.started":"2023-07-02T05:48:57.126695Z","shell.execute_reply":"2023-07-02T06:17:44.994236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the class distribution by dividing the value counts of 'y_' by the total number of samples in 'y_'\ny_.value_counts() / y_.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:17:44.996714Z","iopub.execute_input":"2023-07-02T06:17:44.997684Z","iopub.status.idle":"2023-07-02T06:17:45.007007Z","shell.execute_reply.started":"2023-07-02T06:17:44.997647Z","shell.execute_reply":"2023-07-02T06:17:45.006077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = m.predict_proba(test_pred_and_time)\n#y_pred_list = []\n#for m in models:\n#    y_pred_list.append(m.predict_proba(test_pred_and_time))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:17:45.008681Z","iopub.execute_input":"2023-07-02T06:17:45.009346Z","iopub.status.idle":"2023-07-02T06:22:28.154318Z","shell.execute_reply.started":"2023-07-02T06:17:45.009313Z","shell.execute_reply":"2023-07-02T06:22:28.153348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:22:28.155986Z","iopub.execute_input":"2023-07-02T06:22:28.156352Z","iopub.status.idle":"2023-07-02T06:22:28.165215Z","shell.execute_reply.started":"2023-07-02T06:22:28.156319Z","shell.execute_reply":"2023-07-02T06:22:28.164186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate the first column of 'y_pred' with the sum of the remaining columns along axis 1,\n# and assign the result to the 'probabilities' array\nprobabilities = np.concatenate((y_pred[:, :1], np.sum(y_pred[:, 1:], 1, keepdims=True)), axis=1)\n\n# Extract the first column of 'probabilities' and assign it to the 'p0' array\np0 = probabilities[:, :1]\n\n# Set values in 'p0' that are greater than 0.60 to 1 (True)\np0[p0 > 0.59] = 1\n\n# Set values in 'p0' that are less than 0.25 to 0 (False)\np0[p0 < 0.28] = 0","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:22:28.166422Z","iopub.execute_input":"2023-07-02T06:22:28.167174Z","iopub.status.idle":"2023-07-02T06:22:28.175901Z","shell.execute_reply.started":"2023-07-02T06:22:28.167148Z","shell.execute_reply":"2023-07-02T06:22:28.175073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p0","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:22:28.177384Z","iopub.execute_input":"2023-07-02T06:22:28.17781Z","iopub.status.idle":"2023-07-02T06:22:28.189435Z","shell.execute_reply.started":"2023-07-02T06:22:28.177776Z","shell.execute_reply":"2023-07-02T06:22:28.188486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new DataFrame 'submission' with the 'Id' column from the 'test' DataFrame\nsubmission = pd.DataFrame(test[\"Id\"], columns=[\"Id\"])\n\n# Add a new column 'class_0' to the 'submission' DataFrame containing the values from 'p0'\nsubmission[\"class_0\"] = p0\n\n# Add a new column 'class_1' to the 'submission' DataFrame containing the complement values of 'p0' (1 - p0)\nsubmission[\"class_1\"] = 1 - p0\n\n# Save the 'submission' DataFrame to a CSV file named 'submission.csv' without including the index column\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:22:28.196186Z","iopub.execute_input":"2023-07-02T06:22:28.19647Z","iopub.status.idle":"2023-07-02T06:22:28.206781Z","shell.execute_reply.started":"2023-07-02T06:22:28.196446Z","shell.execute_reply":"2023-07-02T06:22:28.205814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:22:28.208105Z","iopub.execute_input":"2023-07-02T06:22:28.209746Z","iopub.status.idle":"2023-07-02T06:22:28.223801Z","shell.execute_reply.started":"2023-07-02T06:22:28.209714Z","shell.execute_reply":"2023-07-02T06:22:28.222705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.precision', 8)\n\nsubmission['class_1'] = submission['class_1'].apply(lambda x: '{:.8f}'.format(x))\nprint(submission)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T06:22:28.225287Z","iopub.execute_input":"2023-07-02T06:22:28.225918Z","iopub.status.idle":"2023-07-02T06:22:28.235302Z","shell.execute_reply.started":"2023-07-02T06:22:28.225885Z","shell.execute_reply":"2023-07-02T06:22:28.23425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}