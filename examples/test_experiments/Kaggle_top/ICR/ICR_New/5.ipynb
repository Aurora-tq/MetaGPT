{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Import</h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport os\nfrom copy import deepcopy\nfrom functools import partial\nimport random\nimport gc\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Import sklearn classes for model selection, cross validation, and performance evaluation\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport seaborn as sns\nfrom category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Import libraries for Hypertuning\nimport optuna\n\n# Import libraries for gradient boosting\nimport xgboost as xgb\nimport lightgbm as lgb\nimport xgboost as xgb\nimport lightgbm as lgb \nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nfrom catboost import Pool\n\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.inspection import permutation_importance\n\n# Useful line of code to set the display option so we could see all the columns in pd dataframe\npd.set_option('display.max_columns', None)\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-14T19:31:14.610677Z","iopub.execute_input":"2023-07-14T19:31:14.611076Z","iopub.status.idle":"2023-07-14T19:31:14.624362Z","shell.execute_reply.started":"2023-07-14T19:31:14.611046Z","shell.execute_reply":"2023-07-14T19:31:14.62325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Load Data</h1>","metadata":{}},{"cell_type":"code","source":"filepath = '/kaggle/input/icr-identify-age-related-conditions/'\ndf_train = pd.read_csv(os.path.join(filepath, 'train.csv'), index_col=[0])\ndf_test = pd.read_csv(os.path.join(filepath, 'test.csv'), index_col=[0])\ngreeks = pd.read_csv(f'{filepath}greeks.csv')\n\ntarget_col = 'Class'\n\n# df_train['EJ'].replace({'A':0, 'B':1}, inplace=True)\n# df_test['EJ'].replace({'A':0, 'B':1}, inplace=True)\n\ndf_train = df_train.drop(['EJ'], axis=1)\ndf_test = df_test.drop(['EJ'], axis=1)\n\ndf_train = df_train.rename(columns={'BD ': 'BD', 'CD ': 'CD', 'CW ': 'CW', 'FD ': 'FD'})\ndf_test = df_test.rename(columns={'BD ': 'BD', 'CD ': 'CD', 'CW ': 'CW', 'FD ': 'FD'})\n\nprint(f'df_train shape: {df_train.shape}\\n')\nprint(f'df_test shape: {df_test.shape}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:31:14.626183Z","iopub.execute_input":"2023-07-14T19:31:14.626664Z","iopub.status.idle":"2023-07-14T19:31:14.66653Z","shell.execute_reply.started":"2023-07-14T19:31:14.626633Z","shell.execute_reply":"2023-07-14T19:31:14.665502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:31:14.668286Z","iopub.execute_input":"2023-07-14T19:31:14.668714Z","iopub.status.idle":"2023-07-14T19:31:14.731741Z","shell.execute_reply.started":"2023-07-14T19:31:14.668678Z","shell.execute_reply":"2023-07-14T19:31:14.730724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only include numerical features\ndf_train_numerical = df_train.drop(['Class'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:31:14.73306Z","iopub.execute_input":"2023-07-14T19:31:14.733379Z","iopub.status.idle":"2023-07-14T19:31:14.738664Z","shell.execute_reply.started":"2023-07-14T19:31:14.733352Z","shell.execute_reply":"2023-07-14T19:31:14.737811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the KNNImputer with the desired number of neighbors\nimputer = KNNImputer(n_neighbors=20)\n\n# Perform KNN imputation\ndf_train_imputed = pd.DataFrame(imputer.fit_transform(df_train[df_train_numerical.columns]), columns=df_train_numerical.columns)\ndf_test_imputed =pd.DataFrame(imputer.transform(df_test[df_train_numerical.columns]), columns=df_train_numerical.columns)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:31:14.740984Z","iopub.execute_input":"2023-07-14T19:31:14.741546Z","iopub.status.idle":"2023-07-14T19:31:14.779911Z","shell.execute_reply.started":"2023-07-14T19:31:14.741518Z","shell.execute_reply":"2023-07-14T19:31:14.77878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there are still missing values in the train and test data sets\ndf_train_null = df_train_imputed[df_train_imputed.isnull().any(axis=1)]\ndf_test_null = df_test_imputed[df_test_imputed.isnull().any(axis=1)]\n\n# Display the rows with null values\nprint('No. of records with missing value in Train data set after Imputation : {}'.format(df_train_null.shape[0]))\nprint('No. of records with missing value in Test data set after Imputation : {}'.format(df_test_null.shape[0]))\n\nprint('=' * 50)\n\n# Replace the imputed columns in the train data sets\ndf_train_2 = df_train.drop(df_train_numerical.columns, axis=1).reset_index()\ndf_train_2 = pd.concat([df_train_2, df_train_imputed], axis=1)\n\n# Replace the imputed columns in the test data sets\ndf_test_2 = df_test.drop(df_train_numerical.columns, axis=1).reset_index()\ndf_test_2 = pd.concat([df_test_2, df_test_imputed], axis=1)\n\nX_train = df_train_2.drop([f'{target_col}', 'Id'],axis=1).reset_index(drop=True)\ny_train = df_train_2[f'{target_col}'].reset_index(drop=True)\nX_test = df_test_2.drop(['Id'],axis=1).reset_index(drop=True)\n\n# Check the shape of the train and test data set \nprint('Shape of the Train data set : {}'.format(X_train.shape))\nprint('Shape of the Test data set : {}'.format(X_test.shape))\n\nX_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:31:14.781617Z","iopub.execute_input":"2023-07-14T19:31:14.782286Z","iopub.status.idle":"2023-07-14T19:31:14.869674Z","shell.execute_reply.started":"2023-07-14T19:31:14.782246Z","shell.execute_reply":"2023-07-14T19:31:14.868585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_columns = [_ for _ in X_train.columns if _ not in ['EJ']]","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:31:14.871502Z","iopub.execute_input":"2023-07-14T19:31:14.872215Z","iopub.status.idle":"2023-07-14T19:31:14.879574Z","shell.execute_reply.started":"2023-07-14T19:31:14.872174Z","shell.execute_reply":"2023-07-14T19:31:14.878353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_cols = [_ for _ in X_train.columns if _ not in ['EJ', 'BN', 'CW', 'EL', 'GL']]\nX_train.loc[:, log_cols] = np.log1p(X_train.loc[:, log_cols])\nX_test.loc[:, log_cols] = np.log1p(X_test.loc[:, log_cols])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:31:14.880887Z","iopub.execute_input":"2023-07-14T19:31:14.881208Z","iopub.status.idle":"2023-07-14T19:31:14.894644Z","shell.execute_reply.started":"2023-07-14T19:31:14.881181Z","shell.execute_reply":"2023-07-14T19:31:14.893792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_palette = sns.cubehelix_palette(n_colors = 7, start=.46, rot=-.45, dark = .2, hue=0.95)\n# Create subplots\nfig, axes = plt.subplots(len(numeric_columns), 2, figsize=(14, 120))\n\n# Plot the histograms and box plots\nfor i, column in enumerate(numeric_columns):\n    # Histogram\n    sns.histplot(X_train[column], bins=30, kde=True, ax=axes[i, 0], color = my_palette[2])\n    axes[i, 0].set_title(f'Distribution of {column} in df_train')\n    axes[i, 0].set_xlabel('Value')\n    axes[i, 0].set_ylabel('Frequency')\n\n    # Box plot\n    sns.boxplot(X_train[column], ax=axes[i, 1], color = my_palette[1])\n    axes[i, 1].set_title(f'Box plot of {column} in df_train')\n    axes[i, 1].set_xlabel(column)\n    axes[i, 1].set_ylabel('Value')\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-14T19:31:14.896262Z","iopub.execute_input":"2023-07-14T19:31:14.897144Z","iopub.status.idle":"2023-07-14T19:31:39.461631Z","shell.execute_reply.started":"2023-07-14T19:31:14.897105Z","shell.execute_reply":"2023-07-14T19:31:39.460568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler() # MinMaxScaler or StandardScaler\nX_train[numeric_columns] = sc.fit_transform(X_train[numeric_columns])\nX_test[numeric_columns] = sc.transform(X_test[numeric_columns])\n\nprint(f\"X_train shape :{X_train.shape} , y_train shape :{y_train.shape}\")\nprint(f\"X_test shape :{X_test.shape}\")\n\n# Delete the train and test dataframes to free up memory\ndel df_train, df_test, df_train_imputed, df_train_2, df_test_2, df_train_null, df_test_null\n\nX_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:31:39.463053Z","iopub.execute_input":"2023-07-14T19:31:39.463807Z","iopub.status.idle":"2023-07-14T19:31:39.556137Z","shell.execute_reply.started":"2023-07-14T19:31:39.463766Z","shell.execute_reply":"2023-07-14T19:31:39.554967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_palette = sns.cubehelix_palette(n_colors = 7, start=.46, rot=-.45, dark = .2, hue=0.95)\n# Create subplots\nfig, axes = plt.subplots(len(numeric_columns), 2, figsize=(14, 120))\n\n# Plot the histograms and box plots\nfor i, column in enumerate(numeric_columns):\n    # Histogram\n    sns.histplot(X_train[column], bins=30, kde=True, ax=axes[i, 0], color = my_palette[2])\n    axes[i, 0].set_title(f'Distribution of {column} in df_train')\n    axes[i, 0].set_xlabel('Value')\n    axes[i, 0].set_ylabel('Frequency')\n\n    # Box plot\n    sns.boxplot(X_train[column], ax=axes[i, 1], color = my_palette[1])\n    axes[i, 1].set_title(f'Box plot of {column} in df_train')\n    axes[i, 1].set_xlabel(column)\n    axes[i, 1].set_ylabel('Value')\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-07-14T19:31:39.557733Z","iopub.execute_input":"2023-07-14T19:31:39.558144Z","iopub.status.idle":"2023-07-14T19:32:06.807044Z","shell.execute_reply.started":"2023-07-14T19:31:39.558107Z","shell.execute_reply":"2023-07-14T19:32:06.805756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:32:06.808284Z","iopub.execute_input":"2023-07-14T19:32:06.808616Z","iopub.status.idle":"2023-07-14T19:32:06.873326Z","shell.execute_reply.started":"2023-07-14T19:32:06.808587Z","shell.execute_reply":"2023-07-14T19:32:06.872478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n\nclass_weight_0 = 1.0\nclass_weight_1 = 1.0 / scale_pos_weight\n\nclass_weights_cat = [class_weight_0, class_weight_1]\n\nclass_weights_lgb = {0: class_weight_0, 1: class_weight_1}","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:32:06.874649Z","iopub.execute_input":"2023-07-14T19:32:06.874928Z","iopub.status.idle":"2023-07-14T19:32:06.880759Z","shell.execute_reply.started":"2023-07-14T19:32:06.874904Z","shell.execute_reply":"2023-07-14T19:32:06.8799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_log_loss_weight(y_true):\n    nc = np.bincount(y_true)\n    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n    return w0, w1\n\ndef balanced_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n    nc = np.bincount(y_true)\n    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n    balanced_log_loss_score = (-w0/nc[0]*(np.sum(np.where(y_true==0,1,0) * np.log(1-y_pred))) - w1/nc[1]*(np.sum(np.where(y_true!=0,1,0) * np.log(y_pred)))) / (w0+w1)\n    return balanced_log_loss_score","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:32:06.884446Z","iopub.execute_input":"2023-07-14T19:32:06.885353Z","iopub.status.idle":"2023-07-14T19:32:06.897045Z","shell.execute_reply.started":"2023-07-14T19:32:06.885317Z","shell.execute_reply":"2023-07-14T19:32:06.895636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Building Model</h1>","metadata":{}},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, kfold=True, n_splits=5, greeks=pd.DataFrame()):\n        self.n_splits = n_splits\n        self.kfold = kfold\n        self.greeks = greeks\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold == 'skf':\n            for random_state in random_state_list:\n                kf = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    if type(X) is np.ndarray:\n                        X_train, X_val = X[train_index], X[val_index]\n                        y_train, y_val = y[train_index], y[val_index]\n                    else:\n                        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n        else:\n            raise ValueError(f\"Invalid kfold: Must be True\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:32:06.898442Z","iopub.execute_input":"2023-07-14T19:32:06.89891Z","iopub.status.idle":"2023-07-14T19:32:06.90804Z","shell.execute_reply.started":"2023-07-14T19:32:06.898872Z","shell.execute_reply":"2023-07-14T19:32:06.907137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier:\n    def __init__(self, n_estimators=100, device=\"cpu\", random_state=42):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.models = self._define_model()\n        self.models_name = list(self._define_model().keys())\n        self.len_models = len(self.models)\n        \n    def _define_model(self):\n        \n        xgb_optuna1 = {\n            #'n_estimators': 900,\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.09641232707445854,\n            'booster': 'gbtree',\n            'lambda': 4.666002223704784,\n            'alpha': 3.708175990751336,\n            'subsample': 0.6100174145229473,\n            'colsample_bytree': 0.5506821152321051,\n            'max_depth': 7,\n            'min_child_weight': 3,\n            'eta': 1.740374368661041,\n            'gamma': 0.007427363662926455,\n            'grow_policy': 'depthwise',\n            'objective': 'binary:logistic',\n            'eval_metric': 'logloss',\n            'verbosity': 0,\n            'random_state': self.random_state,\n            'scale_pos_weight': scale_pos_weight\n        }\n        \n        xgb_optuna2 = {\n            #'n_estimators': 650,\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.012208383405206188,\n            'booster': 'gbtree',\n            'lambda': 0.009968756668882757,\n            'alpha': 0.02666266827121168,\n            'subsample': 0.7097814108897231,\n            'colsample_bytree': 0.7946945784285216,\n            'max_depth': 3,\n            'min_child_weight': 4,\n            'eta': 0.5480204506554545,\n            'gamma': 0.8788654128774149,\n            'scale_pos_weight': 4.71,\n            'objective': 'binary:logistic',\n            'eval_metric': 'logloss',\n            'verbosity': 0,\n            'random_state': self.random_state,\n            'scale_pos_weight': scale_pos_weight\n        }\n\n        xgb_params = {\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.413327571405248,\n            'booster': 'gbtree',\n            'lambda': 0.0000263894617720096,\n            'alpha': 0.000463768723479341,\n            'subsample': 0.237467672874133,\n            'colsample_bytree': 0.618829300507829,\n            'max_depth': 5,\n            'min_child_weight': 9,\n            'eta': 2.09477807126539E-06,\n            'gamma': 0.000847289463422307,\n            'grow_policy': 'depthwise',\n            'n_jobs': -1,\n            'objective': 'binary:logistic',\n            'eval_metric': 'logloss',\n            'scale_pos_weight': scale_pos_weight,\n            'verbosity': 0,\n            'random_state': self.random_state,\n            \n        }\n        \n        xgb_params2 = {\n            'n_estimators': self.n_estimators,\n            'colsample_bytree': 0.5646751146007976,\n            'gamma': 7.788727238356553e-06,\n            'learning_rate': 0.1419865761603358,\n            'max_bin': 824,\n            'min_child_weight': 1,\n            'random_state': 811996,\n            'reg_alpha': 1.6259583347890365e-07,\n            'reg_lambda': 2.110691851528507e-08,\n            'subsample': 0.879020578464637,\n            'objective': 'binary:logistic',\n            'eval_metric': 'logloss',\n            'max_depth': 3,\n            'n_jobs': -1,\n            'verbosity': 0,\n            'random_state': self.random_state,\n            'scale_pos_weight': scale_pos_weight\n        }\n        \n        xgb_params3 = {\n            'n_estimators': self.n_estimators,\n            'random_state': self.random_state,\n            'colsample_bytree': 0.4836462317215041,\n            'eta': 0.05976752607337169,\n            'gamma': 1,\n            'lambda': 0.2976432557733288,\n            'max_depth': 6,\n            'min_child_weight': 1,\n            'n_estimators': 550,\n            'objective': 'binary:logistic',\n            'scale_pos_weight': 4.260162886376033,\n            'subsample': 0.7119282378433924,\n        }\n        \n        xgb_params4 = {\n            'n_estimators': self.n_estimators,\n            'colsample_bytree': 0.8757972257439255,\n            'gamma': 0.11135738771999848,\n            'max_depth': 7,\n            'min_child_weight': 3,\n            'reg_alpha': 0.4833998914998038,\n            'reg_lambda': 0.006223568555619563,\n            'scale_pos_weight': 8,\n            'subsample': 0.7056434340275685,\n            'random_state': self.random_state\n        }\n        \n        xgb_params5 = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 5, \n            'min_child_weight': 2.934487833919741,\n            'learning_rate': 0.11341944575807082, \n            'subsample': 0.9045063514419968,\n            'gamma': 0.4329153382843715,\n            'colsample_bytree': 0.38872702868412506,\n            'colsample_bylevel': 0.8321880031718571,\n            'colsample_bynode': 0.802355707802605,\n            'random_state': self.random_state\n       }\n        \n        if self.device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n       \n        models = {\n            'xgb01': xgb.XGBClassifier(**xgb_optuna1),\n            'xgb02': xgb.XGBClassifier(**xgb_optuna2),\n            'xgb1': xgb.XGBClassifier(**xgb_params),\n            'xgb2': xgb.XGBClassifier(**xgb_params2),\n            'xgb3': xgb.XGBClassifier(**xgb_params3),\n            #'xgb4': xgb.XGBClassifier(**xgb_params4),\n            #'xgb5': xgb.XGBClassifier(**xgb_params5),\n            #add some models with default params to \"simplify\" ensemble\n            'svc': SVC(random_state=self.random_state, probability=True),\n            'brf': BalancedRandomForestClassifier(random_state=self.random_state),\n            #'lr': LogisticRegression(random_state=self.random_state)\n        }\n        \n        return models","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:32:06.909296Z","iopub.execute_input":"2023-07-14T19:32:06.910195Z","iopub.status.idle":"2023-07-14T19:32:06.928117Z","shell.execute_reply.started":"2023-07-14T19:32:06.910159Z","shell.execute_reply":"2023-07-14T19:32:06.926953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state, n_trials=500):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n        self.n_trials = n_trials\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", 1e-14, 1) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n\n        # Calculate the score for the weighted prediction\n        # score = log_loss(y_true, weighted_pred)\n        score = balanced_log_loss(y_true, weighted_pred)\n        return score\n\n    def fit(self, y_true, y_preds):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        pruner = optuna.pruners.HyperbandPruner()\n        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='minimize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds):\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)\n    \n    def weights(self):\n        return self.weights","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:32:06.930023Z","iopub.execute_input":"2023-07-14T19:32:06.930427Z","iopub.status.idle":"2023-07-14T19:32:06.942967Z","shell.execute_reply.started":"2023-07-14T19:32:06.930376Z","shell.execute_reply":"2023-07-14T19:32:06.942122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nkfold = 'skf'\nn_splits = 10\nn_reapts = 10\nrandom_state = 42\nn_estimators = 99999\nearly_stopping_rounds = 99\nverbose = False\ndevice = 'cpu'\n\n# Fix seed\nrandom.seed(random_state)\nrandom_state_list = random.sample(range(9999), n_reapts)\n#random_state_list = [42]\n\n# Initialize an array for storing test predictions\nclassifier = Classifier(n_estimators, device, random_state)\ntest_predss = np.zeros((X_test.shape[0]))\noof_predss = np.zeros((X_train.shape[0], n_reapts))\nensemble_score, ensemble_score_ = [], []\nweights = []\noof_each_predss = []\noof_each_preds = np.zeros((X_train.shape[0], classifier.len_models))\ntest_each_predss = []\ntest_each_preds = np.zeros((X_test.shape[0], classifier.len_models))\ntrained_models = {'xgb':[], 'cat':[]}\nscore_dict = dict(zip(classifier.models_name, [[] for _ in range(classifier.len_models)]))\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits, greeks=greeks.iloc[:,1:-1])\nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n            \n    # Get a set of classifier models\n    classifier = Classifier(n_estimators, device, random_state_list[m])\n    models = classifier.models\n    \n    # Initialize lists to store oof and test predictions for each base model\n    oof_preds = []\n    test_preds = []\n    \n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    for name, model in models.items():\n        if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n            train_w0, train_w1 = calc_log_loss_weight(y_train_)\n            valid_w0, valid_w1 = calc_log_loss_weight(y_val)\n            if 'xgb' in name:\n                model.fit(\n                    X_train_, y_train_, sample_weight=y_train_.map({0: train_w0, 1: train_w1}), \n                    eval_set=[(X_val, y_val)], sample_weight_eval_set=[y_val.map({0: valid_w0, 1: valid_w1})],\n                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            elif 'lgb' in name:\n                model.fit(\n                    X_train_, y_train_, sample_weight=y_train_.map({0: train_w0, 1: train_w1}), \n                    eval_set=[(X_val, y_val)], eval_sample_weight=[y_val.map({0: valid_w0, 1: valid_w1})],\n                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            elif 'cat' in name:\n                model.fit(\n                    Pool(X_train_, y_train_, weight=y_train_.map({0: train_w0, 1: train_w1})), \n                    eval_set=Pool(X_val, y_val, weight=y_val.map({0: valid_w0, 1: valid_w1})), \n                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        else:\n            model.fit(X_train_, y_train_)\n            \n        if name in trained_models.keys():\n            trained_models[f'{name}'].append(deepcopy(model))\n        \n        test_pred = model.predict_proba(X_test)[:, 1].reshape(-1)\n        y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1)\n        \n        # Calculate recall and precision scores\n        y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n        recall = recall_score(y_val, y_val_pred_binary)\n        precision = precision_score(y_val, y_val_pred_binary)\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] Recall score: {recall:.5f}')\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] Precision score: {precision:.5f}')\n\n        score = balanced_log_loss(y_val, y_val_pred)\n        score_dict[name].append(score)\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] BalancedLogLoss score: {score:.5f}')\n        print('-'*50)\n        \n        oof_preds.append(y_val_pred)\n        test_preds.append(test_pred)\n    \n    # Use Optuna to find the best ensemble weights\n    optweights = OptunaWeights(random_state=random_state_list[m])\n    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n    \n    score = balanced_log_loss(y_val, y_val_pred)\n    score_ = roc_auc_score(y_val, y_val_pred)\n    print(f'--> Ensemble [FOLD-{n} SEED-{random_state_list[m]}] BalancedLogLoss score {score:.5f}')\n    print('='*50)\n    ensemble_score.append(score)\n    ensemble_score_.append(score_)\n    weights.append(optweights.weights)\n    \n    # Predict to X_test by the best ensemble weights\n    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n    oof_predss[X_val.index, m] += optweights.predict(oof_preds)\n    oof_each_preds[X_val.index] = np.stack(oof_preds).T\n    test_each_preds += np.array(test_preds).T / n_splits\n    if n == (n_splits - 1):\n        oof_each_predss.append(oof_each_preds)\n        oof_each_preds = np.zeros((X_train.shape[0], classifier.len_models))\n        test_each_predss.append(test_each_preds)\n        test_each_preds = np.zeros((X_test.shape[0], classifier.len_models))\n    \n    gc.collect()\n    \noof_each_predss = np.mean(np.array(oof_each_predss), axis=0)\ntest_each_predss = np.mean(np.array(test_each_predss), axis=0)\noof_each_predss = np.concatenate([oof_each_predss, np.mean(oof_predss, axis=1).reshape(-1, 1)], axis=1)\ntest_each_predss = np.concatenate([test_each_predss, test_predss.reshape(-1, 1)], axis=1)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-14T19:32:06.944233Z","iopub.execute_input":"2023-07-14T19:32:06.944678Z","iopub.status.idle":"2023-07-14T19:41:34.390341Z","shell.execute_reply.started":"2023-07-14T19:32:06.944651Z","shell.execute_reply":"2023-07-14T19:41:34.389281Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean score of the ensemble\nmean_score = np.mean(ensemble_score)\nstd_score = np.std(ensemble_score)\nprint(f'Mean Optuna Ensemble {mean_score:.5f} ± {std_score:.5f} \\n')\n\nprint('--- Optuna Weights---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:41:34.391841Z","iopub.execute_input":"2023-07-14T19:41:34.392221Z","iopub.status.idle":"2023-07-14T19:41:34.399977Z","shell.execute_reply.started":"2023-07-14T19:41:34.392194Z","shell.execute_reply":"2023-07-14T19:41:34.399081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_palette = sns.cubehelix_palette(n_colors = 7, start=.46, rot=-.45, dark = .2, hue=0.95, as_cmap=True)\n\ndef show_confusion_roc(oof, title='Model Evaluation Results'):\n    f, ax = plt.subplots(1, 2, figsize=(16, 6))\n    df = pd.DataFrame({'preds': oof[0], 'target': oof[1]})\n    cm = confusion_matrix(df.target, df.preds.ge(0.5).astype(int))\n    cm_display = ConfusionMatrixDisplay(cm).plot(cmap=my_palette, ax=ax[0])\n    ax[0].grid(False)\n    RocCurveDisplay.from_predictions(df.target, df.preds, ax=ax[1])\n    ax[1].grid(True)\n    plt.suptitle(f'{title}', fontsize=12, fontweight='bold')\n    plt.tight_layout()\n\nshow_confusion_roc(oof=[oof_each_predss[:, 0], y_train], title='OOF Evaluation Results')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:41:34.401153Z","iopub.execute_input":"2023-07-14T19:41:34.401666Z","iopub.status.idle":"2023-07-14T19:41:34.94268Z","shell.execute_reply.started":"2023-07-14T19:41:34.401631Z","shell.execute_reply":"2023-07-14T19:41:34.941645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nstack_test_predss = np.zeros((X_test.shape[0]))\nstack_scores = []\nstack_models = []\nsplitter = Splitter(kfold=kfold, n_splits=n_splits, greeks=greeks.iloc[:,1:-1])\nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(oof_each_predss, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n    \n    classifier = Classifier(n_estimators, device, random_state_list[m])\n    models = classifier.models\n    model = models['xgb02']\n    \n    train_w0, train_w1 = calc_log_loss_weight(y_train_)\n    valid_w0, valid_w1 = calc_log_loss_weight(y_val)\n    \n    model.fit(\n    X_train_, y_train_, sample_weight=y_train_.map({0: train_w0, 1: train_w1}),\n    eval_set=[(X_val, y_val)],\n   # eval_metric='logloss',\n    sample_weight_eval_set=[y_val.map({0: valid_w0, 1: valid_w1})],\n    early_stopping_rounds=early_stopping_rounds,\n    verbose=verbose\n)\n    \n    test_pred = model.predict_proba(test_each_predss)[:, 1].reshape(-1)\n    y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1)\n\n    score = balanced_log_loss(y_val, y_val_pred)\n    stack_scores.append(score)\n    stack_models.append(deepcopy(model))\n    \n    stack_test_predss += test_pred / (n_splits * len(random_state_list))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:41:34.944563Z","iopub.execute_input":"2023-07-14T19:41:34.945307Z","iopub.status.idle":"2023-07-14T19:42:07.887551Z","shell.execute_reply.started":"2023-07-14T19:41:34.945267Z","shell.execute_reply":"2023-07-14T19:42:07.886389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean LogLoss score of the ensemble\nmean_score = np.mean(ensemble_score)\nstd_score = np.std(ensemble_score)\nprint(f'Ensemble BalancedLogLoss score {mean_score:.5f} ± {std_score:.5f}')\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')\nprint('')\n\n# Calculate the mean LogLoss score of the ensemble\nmean_score = np.mean(stack_scores)\nstd_score = np.std(stack_scores)\nprint(f'Stacking BalancedLogLoss score {mean_score:.5f} ± {std_score:.5f}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:42:07.888606Z","iopub.execute_input":"2023-07-14T19:42:07.888888Z","iopub.status.idle":"2023-07-14T19:42:07.897716Z","shell.execute_reply.started":"2023-07-14T19:42:07.888863Z","shell.execute_reply":"2023-07-14T19:42:07.896837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Submission</h1>","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\n\nsub['class_1'] = stack_test_predss\nsub['class_0'] = 1 - stack_test_predss\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:42:07.899005Z","iopub.execute_input":"2023-07-14T19:42:07.899831Z","iopub.status.idle":"2023-07-14T19:42:07.928719Z","shell.execute_reply.started":"2023-07-14T19:42:07.899792Z","shell.execute_reply":"2023-07-14T19:42:07.927716Z"},"trusted":true},"execution_count":null,"outputs":[]}]}